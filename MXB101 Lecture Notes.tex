%!TEX program = xelatex
\documentclass{article}
\input{LaTeX-Submodule/template.tex}

% Additional packages & macros
\usepackage{multicol}

% Header and footer
\newcommand{\unitName}{Probability and Stochastic Modelling 1}
\newcommand{\unitTime}{Semester 1, 2022}
\newcommand{\unitCoordinator}{Dr Alexander Browning}
\newcommand{\documentAuthors}{\textsc{Tarang Janawalkar}}

\fancyhead[L]{\unitName}
\fancyhead[R]{\leftmark}
\fancyfoot[C]{\thepage}

% Copyright
\usepackage[
    type={CC},
    modifier={by-nc-sa},
    version={4.0},
    imagewidth={5em},
    hyphenation={raggedright}
]{doclicense}

\date{}

\begin{document}
%
\begin{titlepage}
    \vspace*{\fill}
    \begin{center}
        \LARGE{\textbf{\unitName}} \\[0.1in]
        \normalsize{\unitTime} \\[0.2in]
        \normalsize\textit{\unitCoordinator} \\[0.2in]
        \documentAuthors
    \end{center}
    \vspace*{\fill}
    \doclicenseThis
    \thispagestyle{empty}
\end{titlepage}
\newpage
%
\tableofcontents
\newpage
%
\section{Events and Probability}
\subsection{Events}
\begin{definition}[Event]
    An event is a set of outcomes in a random experiment commonly denoted by a capital letter.
    Events can be simple (a single event) or compound (two or more simple events).
\end{definition}
\begin{definition}[Sample space]
    The set of all possible outcomes of an experiment is known as the sample space
    for that experiment and is denoted \(\Omega\).
\end{definition}
\begin{definition}[Intersection]
    An intersection between two events \(A\) and \(B\) describes the set of outcomes that occur in both \(A\) and \(B\).
    The intersection can be represented using the set {\ttfamily{AND}} operator (\(\cap\)) --- \(A \cap B\) (or \(AB\)).
\end{definition}
\begin{definition}[Disjoint]
    Disjoint (mutually exclusive) events are two events that cannot occur simultaneously or have no common outcomes.
\end{definition}
\begin{theorem}[Intersection of disjoint events]
    The intersection of disjoint events results in the null set (\(\varnothing\)).
\end{theorem}
\begin{lemma}
    Disjoint events are \textbf{dependent} events as the occurrence of one means the other cannot occur.
\end{lemma}
\begin{definition}[Union]
    A union of two events \(A\) and \(B\) describes the set of outcomes in either \(A\) or \(B\).
    The union is represented using the set {\ttfamily{OR}} operator (\(\cup\)) --- \(A \cup B\).
\end{definition}
\begin{definition}[Complement]
    The complement of an event \(E\) is the set of all other outcomes in \(\Omega\).
    The complement of \(E\) is denoted \(\overline{E}\).
\end{definition}
\begin{theorem}[Intersection of complement set]
    \begin{equation*}
        A\overline{A} = \varnothing
    \end{equation*}
\end{theorem}
\begin{theorem}[Union of complement set]
    \begin{equation*}
        A \cup \overline{A} = \Omega
    \end{equation*}
\end{theorem}
\begin{definition}[Subset]
    \(A\) is a (non-strict) subset of \(B\) if all elements in \(A\) are also in \(B\).
    This can be denoted as \(A \subset B\).
\end{definition}
\begin{theorem}
    All events \(E\) are subsets of \(\Omega\).
\end{theorem}
\begin{theorem}
    Given \(A \subset B\)
    \begin{equation*}
        AB = A \quad\quad \text{and} \quad\quad A \cup B = B
    \end{equation*}
\end{theorem}
\begin{corollary}
    Given \(\varnothing \subset E\)
    \begin{equation*}
        \varnothing E = \varnothing \quad\quad \text{and} \quad\quad \varnothing \cup E = E
    \end{equation*}
\end{corollary}
\begin{theorem}[Associative Identities]
    \begin{align*}
        A \left( BC \right)            & = \left( AB \right) C            \\
        A \cup \left( B \cup C \right) & = \left( A \cup B \right) \cup C
    \end{align*}
\end{theorem}
\begin{theorem}[Distributive Identities]
    \begin{align*}
        A \left(B \cup C\right) & = AB \cup AC                                      \\
        A \cup BC               & = \left( A \cup B \right) \left( A \cup C \right)
    \end{align*}
\end{theorem}
\subsection{Probability}
\begin{definition}[Probability]
    Probability is a measure of the likeliness of an event occurring. The probability of
    an event \(E\) is denoted \(\Pr{\left( E \right)}\) (sometimes \(\mathrm{P}\left( E \right)\)).
    \begin{equation*}
        0 \leq \Pr{\left( E \right)} \leq 1
    \end{equation*}
    where a probability of 0 never happens, and 1 always happens.
\end{definition}
\begin{theorem}[Probability of \(\Omega\)]
    \begin{equation*}
        \Pr{\left( \Omega \right)} = 1
    \end{equation*}
\end{theorem}
\begin{theorem}[Complement rule]
    The probability of the complement of \(E\) is given by
    \begin{equation*}
        \Pr{\left( \overline{E} \right)} = 1 - \Pr{\left( E \right)}
    \end{equation*}
\end{theorem}
\begin{theorem}[Multiplication rule for independent events]
    The probability of the intersection between two independent events \(A\) and \(B\) is given by
    \begin{equation*}
        \Pr{\left( AB \right)} = \Pr{\left( A \right)} \Pr{\left( B \right)}
    \end{equation*}
\end{theorem}
\begin{theorem}[Addition rule for independent events]
    The probability of the union between two independent events \(A\) and \(B\) is given by
    \begin{equation*}
        \Pr{\left( A \cup B \right)} = \Pr{\left( A \right)} + \Pr{\left( B \right)} - \Pr{\left( AB \right)}.
    \end{equation*}
    If \(A\) and \(B\) are disjoint, then \(\Pr{\left( AB \right)} = 0\), so that \(\Pr{\left( A \cup B \right)} = \Pr{\left( A \right)} + \Pr{\left( B \right)}\).
\end{theorem}
\begin{corollary}[Addition rule for 3 events]
    The addition rule for 3 events is as follows
    \begin{equation*}
        \Pr{\left( A \cup B \cup C \right)} = \Pr{\left( A \right)} + \Pr{\left( B \right)} + \Pr{\left( C \right)} - \Pr{\left( AB \right)} - \Pr{\left( AC \right)} - \Pr{\left( BC \right)} + \Pr{\left( ABC \right)}.
    \end{equation*}
\end{corollary}
\begin{proof}
    If we write \(D = A \cup B\) and apply the addition rule twice, we have
    \begin{align*}
        \Pr{\left( A \cup B \cup C \right)} & = \Pr{\left( D \cup C \right)}                                                                                                                                                               \\
                                            & = \Pr{\left( D \right)} + \Pr{\left( C \right)} - \Pr{\left( DC \right)}                                                                                                                     \\
                                            & = \Pr{\left( A \cup B \right)} + \Pr{\left( C \right)} - \Pr{\left( \left( A \cup B \right)C \right)}                                                                                        \\
                                            & = \Pr{\left( A \right)} + \Pr{\left( B \right)} - \Pr{\left( AB \right)} + \Pr{\left( C \right)} - \Pr{\left( AC \cup BC \right)}                                                            \\
                                            & = \Pr{\left( A \right)} + \Pr{\left( B \right)} - \Pr{\left( AB \right)} + \Pr{\left( C \right)} - \left( \Pr{\left( AC \right)} + \Pr{\left( BC \right)} - \Pr{\left( ACBC \right)} \right) \\
                                            & = \Pr{\left( A \right)} + \Pr{\left( B \right)} + \Pr{\left( C \right)} - \Pr{\left( AB \right)} - \Pr{\left( AC \right)} - \Pr{\left( BC \right)} + \Pr{\left( ABC \right)}
    \end{align*}
\end{proof}
\begin{theorem}[De Morgan's laws]
    Recall De Morgan's Laws:
    \begin{align*}
        \overline{A \cup B} & = \overline{A} \ \overline{B}     \\
        \overline{AB}       & = \overline{A} \cup \overline{B}.
    \end{align*}
    Taking the negation of both sides and applying the complement rule yields
    \begin{align*}
        \Pr{\left( A \cup B \right)} & = 1 - \Pr{\left( \overline{A} \ \overline{B} \right)}    \\
        \Pr{\left( AB \right)}       & = 1 - \Pr{\left( \overline{A} \cup \overline{B} \right)}
    \end{align*}
\end{theorem}
\subsection{Circuits}
A signal can pass through a circuit if there is a functional path from start to finish.

We can define a circuit where each component \(i\) functions with probability \(p\),
and is independent of other components.

Then \(W_i\) to be the event in which the associated component \(i\) functions, we can
determine the event \(S\) in which the system functions,
and probability \(\Pr{\left( S \right)}\) that the system functions.

As the probability that any component functions is \(p\), in other words
\begin{equation*}
    \Pr{\left( W_i \right)} = p,
\end{equation*}
\(\Pr{\left( S \right)}\) will be a function of \(p\) defined \(f:\left[ 0,\; 1 \right] \to \left[ 0,\; 1 \right]\).
\section{Independence}
\begin{definition}[Conditional probability]
    When discussing multiple events, it is possible that the occurrence of one event changes
    the probability that another will occur. This can be denoted using a vertical bar,
    and is read as ``the probability of event \(A\) given \(B\)'':
    \begin{equation*}
        \Pr{\left( A \,\vert\, B \right)} = \frac{\Pr{\left( A B \right)}}{\Pr{\left( B \right)}}.
    \end{equation*}
\end{definition}
\begin{definition}[Multiplication rule]
    For events \(A\) and \(B\), the general multiplication rule states that
    \begin{equation*}
        \Pr{\left( A B \right)} = \Pr{\left( A \,\vert\, B \right)} \Pr{\left( B \right)}
    \end{equation*}
\end{definition}
\begin{theorem}[Independent events]
    If \(A\) and \(B\) are independent events then
    \begin{align*}
        \Pr{\left( A \,\vert\, B \right)} & = \Pr{\left( A \right)} \\
        \Pr{\left( B \,\vert\, A \right)} & = \Pr{\left( B \right)}
    \end{align*}
\end{theorem}
\begin{theorem}[Complement of independent events]
    If \(A\) and \(B\) are independent, all complement pairs are also independent.
    Given
    \begin{align*}
        \Pr{\left( A \,\vert\, B \right)} & = \Pr{\left( A \right)} \\
        \Pr{\left( B \,\vert\, A \right)} & = \Pr{\left( B \right)}
    \end{align*}
    the following statements are also true
    \begin{align*}
        \Pr{\left( A \,\vert\, \overline{B} \right)}            & = \Pr{\left( A \right)}            & \Pr{\left( B \,\vert\, \overline{A} \right)}            & = \Pr{\left( B \right)}            \\
        \Pr{\left( \overline{A} \,\vert\, B \right)}            & = \Pr{\left( \overline{A} \right)} & \Pr{\left( \overline{B} \,\vert\, A \right)}            & = \Pr{\left( \overline{B} \right)} \\
        \Pr{\left( \overline{A} \,\vert\, \overline{B} \right)} & = \Pr{\left( \overline{A} \right)} & \Pr{\left( \overline{B} \,\vert\, \overline{A} \right)} & = \Pr{\left( \overline{B} \right)}
    \end{align*}
\end{theorem}
\subsection{Probability Rules with Conditional}
ALl probability rules hold when conditioning on some event \(C\).
\begin{theorem}[Complement rule with condition]
    \begin{equation*}
        \Pr{\left( \overline{A} \,\vert\, C \right)} = 1 - \Pr{\left( A \,\vert\, C \right)}
    \end{equation*}
\end{theorem}
\begin{theorem}[Addition rule with condition]
    \begin{equation*}
        \Pr{\left( A \cup B \,\vert\, C \right)} = \Pr{\left( A \,\vert\, C \right)} + \Pr{\left( B \,\vert\, C \right)} - \Pr{\left( AB \,\vert\, C \right)}
    \end{equation*}
\end{theorem}
\begin{theorem}[Multiplication rule with condition]
    \begin{equation*}
        \Pr{\left( A B \,\vert\, C \right)} = \Pr{\left( A \,\vert\, BC \right)} \Pr{\left( B \,\vert\, C \right)}
    \end{equation*}
\end{theorem}
In the above examples, all probabilities are conditional on the sample space, hence we are effectively
changing the sample space.
\subsection{Conditional Independence}
\begin{definition}[Conditional independence]
    Suppose events \(A\) and \(B\) are not independent, i.e.,
    \begin{equation*}
        \Pr{\left( A \,\vert\, B \right)} \neq \Pr{\left( A \right)}
    \end{equation*}
    but they become independent when conditioned with another event \(C\), i.e.,
    \begin{equation*}
        \Pr{\left( A \,\vert\, BC \right)} = \Pr{\left( A \,\vert\, C \right)}
    \end{equation*}
    Here we say that \(A\) and \(B\) are \textbf{conditionally independent} given \(C\). Furthermore
    \begin{equation*}
        \Pr{\left( AB \,\vert\, C \right)} = \Pr{\left( A \,\vert\, C \right)} \Pr{\left( B \,\vert\, C \right)}
    \end{equation*}
    Conversely, events \(A\) and \(B\) may be conditionally dependent but unconditionally independent, i.e.,
    \begin{align*}
        \Pr{\left( A \,\vert\, B \right)}  & = \Pr{\left( A \right)}                                                \\
        \Pr{\left( A \,\vert\, BC \right)} & \neq \Pr{\left( A \,\vert\, C \right)}                                 \\
        \Pr{\left( AB \,\vert\, C \right)} & = \Pr{\left( A \,\vert\, BC \right)} \Pr{\left( B \,\vert\, C \right)}
    \end{align*}
\end{definition}
\begin{theorem}
    Given events \(A\), \(B\), and \(C\). Pairwise independence does not imply mutual independence. I.e.,
    \begin{equation*}
        \begin{cases}
            \Pr{\left( A B \right)} = \Pr{\left( A \right)} \Pr{\left( B \right)} \\
            \Pr{\left( A C \right)} = \Pr{\left( A \right)} \Pr{\left( C \right)} \\
            \Pr{\left( B C \right)} = \Pr{\left( B \right)} \Pr{\left( C \right)}
        \end{cases}
    \end{equation*}
    does not imply
    \begin{equation*}
        \Pr{\left( A B C \right)} = \Pr{\left( A \right)} \Pr{\left( B \right)} \Pr{\left( C \right)}.
    \end{equation*}
\end{theorem}
In summary, independence should not be assumed unless explicitly stated.
\subsection{Disjoint Events}
\begin{theorem}[Probability of disjoint events]
    The probability of disjoint events \(A\) and \(B\) is given by
    \begin{align*}
        \Pr{\left( AB \right)}          & = 0  \\
        \Pr{\left( \varnothing \right)} & = 0.
    \end{align*}
    Disjoint events are highly dependent events, since the occurrence of one means the other cannot occur.
    This implies
    \begin{equation*}
        \Pr{\left( A \,\vert\, B \right)} = 0
    \end{equation*}
\end{theorem}
\subsection{Subsets}
\begin{theorem}[Probability of subsets]
    If \(A \subset B\) then \(\Pr{\left( A \right)} \leq \Pr{\left( B \right)}\).
    We also know that \(\Pr{\left( AB \right)} = \Pr{\left( A \right)}\) and \(\Pr{\left( A \cup B \right)} = \Pr{\left( B \right)}\).

    Here, if \(A\) happens, then \(B\) definitely happens.
    \begin{equation*}
        \Pr{\left( B \,\vert\, A \right)} = 1
    \end{equation*}
    Given \(\Pr{\left( AB \right)} = \Pr{\left( A \right)}\)
    \begin{equation*}
        \Pr{\left( A \,\vert\, B \right)} = \frac{\Pr{\left( A \right)}}{\Pr{\left( B \right)}}
    \end{equation*}
    These events are also highly dependent.
\end{theorem}
\section{Total Probability}
\begin{definition}[Marginal probability]
    Marginal probability is the probability of an event \linebreak irrespective of the outcome of another variable.
\end{definition}
\begin{theorem}[Total probability for complements]
    By writing the event \(A\) as \(AB \cup A\overline{B}\), and noting that \(AB\) and \(A\overline{B}\) are disjoint,
    the marginal probability of \(A\) is given by
    \begin{equation*}
        \Pr{\left( A \right)} = \Pr{\left( AB \right)} + \Pr{\left( A\overline{B} \right)}.
    \end{equation*}
    By applying the multiplication rule to each joint probability:
    \begin{equation*}
        \Pr{\left( A \right)} = \Pr{\left( A \,\vert\, B \right)}\Pr{\left( B \right)} + \Pr{\left( A \,\vert\, \overline{B} \right)}\Pr{\left( \overline{B} \right)}
    \end{equation*}
\end{theorem}
\begin{theorem}[Law of total probability]
    The previous theorem partitioned \(\Omega\) into disjoint events \(B\) and \(\overline{B}\).

    By partitioning \(\Omega\) into a collection of disjoint events \(B_1,\; B_2,\; \dots,\; B_n\),
    such that \(\bigcup_{i=1}^n B_i = \Omega\), we have
    \begin{equation*}
        \Pr{\left( A \right)} = \sum_{i = 1}^n \Pr{\left( A \,\vert\, B_i \right)}\Pr{\left( B_i \right)}
    \end{equation*}
\end{theorem}
\begin{theorem}[Bayes' Theorem]
    Given the probability for \(A\) given \(B\), the probability of the reverse direction is given by
    \begin{equation*}
        \Pr{\left( A \,\vert\, B \right)} = \frac{\Pr{\left( B \,\vert\, A \right)}\Pr{\left( A \right)}}{\Pr{\left( B \right)}}
    \end{equation*}
\end{theorem}
\section{Combinatorics}
\begin{definition}[Number of outcomes]
    Let \(\abs{A}\) denote the number of outcomes in an event \(A\).
\end{definition}
\begin{theorem}[Addition principle]
    Given a sample space \(S\) with \(k\) disjoint events \({\left\{ S_1,\:\ldots,\:S_k \right\}}\),
    where the \(i\)th event has \(n_i\) possible outcomes,
    the number of possible samples from any event is given by
    \begin{equation*}
        \abs{\bigcup_{i = 0}^{k} S_i} = \sum_{i = 1}^k n_i
    \end{equation*}
\end{theorem}
\begin{theorem}[Multiplication principle]
    Given a sample space \(S\) with \(k\) events \({\left\{ S_1,\:\ldots,\:S_k \right\}}\),
    where the \(i\)th event has \(n_i\) possible outcomes,
    the number of possible samples from every event is given by
    \begin{equation*}
        \abs{\bigcap_{i=0}^{k} S_i} = \prod_{i = 1}^k n_i
    \end{equation*}
\end{theorem}
\begin{theorem}[Counting probability]
    Given a sample space \(S\) with equally likely outcomes, the probability
    of an event \(S_i \subset S\) is given by
    \begin{equation*}
        \Pr{\left( S_i \right)} = \frac{\abs{S_i}}{\abs{S}}
    \end{equation*}
\end{theorem}
\subsection{Ordered Sampling with Replacement}
When ordering is important and repetition is allowed,
the total number of ways to choose \(k\) objects from a set with \(n\) elements is
\begin{equation*}
    n^k
\end{equation*}
\subsection{Ordered Sampling without Replacement}
When ordering is important and repetition is not allowed,
the total number of ways to arrange \(k\) objects from a set of \(n\) elements is
known as a \(k\)-permutation of \(n\)-elements denoted \(\prescript{n}{}{P}_k\)
\begin{align*}
    \prescript{n}{}{P}_k & = n \times \left( n - 1 \right) \times \cdots \times \left( n - k + 1 \right) \\
                         & = \frac{n!}{\left( n - k \right)!}
\end{align*}
for \(0 \leq k \leq n\).
\begin{definition}[Permutation of \(n\) elements]
    An \(n\)-permutation of \(n\) elements is the permutation of those elements.
    In this case, \(k = n\), so that
    \begin{align*}
        \prescript{n}{}{P}_n & = n \times \left( n - 1 \right) \times \cdots \times \left( n - n + 1 \right) \\
                             & = n!
    \end{align*}
\end{definition}
\subsection{Unordered Sampling without Replacement}
When ordering is not important and repetition is not allowed,
the total number of ways to choose \(k\) objects from a set of \(n\) elements is
known as a \(k\)-combination of \(n\)-elements denoted \(\prescript{n}{}{C}_k\) or \(\binom{n}{k}\)
\begin{align*}
    \prescript{n}{}{C}_k & = \frac{\prescript{n}{}{P}_k}{k!}     \\
                         & = \frac{n!}{k! \left( n - k \right)!}
\end{align*}
for \(0 \leq k \leq n\). We divide by \(k!\) because any \(k\)-element subset of \(n\)-elements % chktex 40
can be ordered in \(k!\) ways. % chktex 40
\subsection{Unordered Sampling with Replacement}
When ordering is not important and repetition is allowed,
the total number of ways to choose \(k\) objects from a set with \(n\) elements is
\begin{equation*}
    \binom{n + k - 1}{k}
\end{equation*}
\section{Random Variables and Distributions}
\section{Random Variables}
\begin{definition}[Random variable]
    A random variable \(X\) is a measurable variable whose value holds some uncertainty.
\end{definition}
An event is when a random variable assumes a certain value or range of values.
\begin{definition}[Discrete random variables]
    A discrete random variable takes discrete values.
\end{definition}
\begin{definition}[Continuous random variables]
    A continuous random variable can take any real value.
\end{definition}
\subsection{Probability Distributions}
\begin{definition}[Probability distribution]
    The probability distribution of a random variable \(X\) is a function that links all outcomes \(x \in \Omega\)
    to the probability that they will occur \(\Pr{\left( X = x \right)}\).
\end{definition}
\begin{definition}[Probability mass function]
    The probability distribution of a discrete random variable \(X\) is described by a Probability
    Mass Function (PMF) \(p_x\).
    \begin{equation*}
        \Pr{\left( X = x \right)} = p_x
    \end{equation*}
    \(p_x\) is a valid PMF provided,
    \begin{align*}
        \forall x \in \Omega : \Pr{\left( X = x \right)} & \geq 0 &  & \text{and} & \sum_\Omega \Pr{\left( X = x \right)} & = 1.
    \end{align*}
\end{definition}
\begin{definition}[Probability density function]
    The probability distribution of a continuous random variable \(X\) is described by a Probability
    Density Function (PDF) \(f\left( x \right)\).

    The probability that \(X\) is exactly equal to a
    specific value is always 0. Therefore we compute probabilities over intervals:
    \begin{equation*}
        \Pr{\left( x_1 \leq X \leq x_2 \right)} = \int_{x_1}^{x_2} f\left( u \right) \odif{u}
    \end{equation*}
    \(f\left( x \right)\) is a valid PDF provided,
    \begin{align*}
        \forall x \in \Omega : f\left( x \right) & \geq 0 &  & \text{and} & \int_{\Omega} f(u) \odif{u} & = 1.
    \end{align*}
\end{definition}
\begin{definition}[Cumulative distribution function]
    The Cumulative Distribution Function (CDF) computes the probability that the random variable is
    less than or equal to a particular realisation \(x\). For \(U = \left\{ k \in \Omega : k \leq x \right\}\)
    \begin{equation*}
        F\left( x \right) = \Pr{\left( X \leq x \right)} = \begin{cases}
            \displaystyle \sum_{u \in U} p_u                  & \text{for discrete random variables}    \\[0.4cm]
            \displaystyle \int_{U} f\left( u \right) \odif{u} & \text{for continuous random variables}.
        \end{cases}
    \end{equation*}
    \(F\left( x \right)\) is a valid CDF if:
    \begin{enumerate}
        \item \(F\) is monotonically increasing and continuous
        \item \(\lim_{x \to -\infty} F\left( x \right) = 0\)
        \item \(\lim_{x \to \infty} F\left( x \right) = 1\)
    \end{enumerate}
    We can recover the PDF given the CDF, by using the Fundamental Theorem of Calculus.
    \begin{equation*}
        \odv{F\left( x \right)}{x} = \odv{}{x} \int_{-\infty}^x f\left( u \right) \odif{u} = f\left( x \right)
    \end{equation*}
\end{definition}
\begin{definition}[Complementary CDF]
    For a continuous random variable \(X\) the complement function,
    \begin{equation*}
        \Pr{\left( X > x \right)} = 1 - \Pr{\left( X \leq x \right)} = 1 - F\left( x \right)
    \end{equation*}
    is called the complementary CDF, or the survival function.
\end{definition}
\subsection{Quantiles}
\begin{definition}[\(p\)-Quantile]
    For a continuous random variable, the \(p\)-quantile, \(x\), is defined such that
    \begin{equation*}
        F\left( x \right) = \int_{-\infty}^x f\left( u \right) \odif{u} = p.
    \end{equation*}
\end{definition}
\begin{definition}[Median]
    The median, \(m\), is a special \(p\)-quartile defined as the value such that
    \begin{equation*}
        \int_{-\infty}^m f\left( u \right) \odif{u} = \int^{\infty}_m f\left( u \right) \odif{u} = \frac{1}{2}.
    \end{equation*}
\end{definition}
\begin{definition}[Lower and upper quartile]
    Likewise the lower quartile and upper quartiles are two values \(q_1\) and \(q_2\) such that
    \begin{equation*}
        \int_{-\infty}^{q_1} f\left( u \right) \odif{u} = \frac{1}{4}
    \end{equation*}
    and
    \begin{equation*}
        \int_{-\infty}^{q_2} f\left( u \right) \odif{u} = \frac{3}{4}.
    \end{equation*}
\end{definition}
\begin{definition}[Quantile function]
    The quantile function is the inverse of the CDF
    and can be used to find the \(x\) that a certain \(p\) provides. I.e.,
    \begin{equation*}
        x = F^{-1}\left( p \right) = Q\left( p \right)
    \end{equation*}
\end{definition}
\subsection{Summary Statistics}
\begin{definition}[Expectation]
    The expectation \(\E{\left( X \right)}\), or \(\operatorname{\mathbb{E}}{\left( X \right)}\)
    of a random variable \(X\) is its expected value given an
    infinite number of observations.

    The expectation is also known as the mean of the \(X\), denoted \(\mu\).
    \begin{equation*}
        \E{\left( X \right)} =
        \begin{cases}
            \displaystyle \sum_{x \in \Omega} x p_x                & \text{for discrete variables}   \\[0.4cm]
            \displaystyle \int_\Omega x f\left( x \right) \odif{x} & \text{for continuous variables}
        \end{cases}
    \end{equation*}
\end{definition}
\begin{theorem}
    Using integration by parts, it can be proved that
    \begin{equation*}
        \E{\left(X\right)} = -\int_{-\infty}^0 F\left( x \right) \odif{x} + \int_0^\infty \left(1 - F\left( x \right)\right) \odif{x}
    \end{equation*}
\end{theorem}
\begin{definition}[Variance]
    The variance \(\Var{\left( X \right)}\), or \(\operatorname{\mathbb{V}}{\left( X \right)}\) of a random variable \(X\) is a measure of spread
    of the distribution (defined as the average squared distance of each value from the mean).
    Variance is also denoted as \(\sigma^2\).
    \begin{align*}
        \Var{\left( X \right)} & =
        \begin{cases}
            \displaystyle \sum_{x \in \Omega} \left( x - \mu \right)^2 p_x                  & \text{for discrete variables}   \\[0.4cm]
            \displaystyle \int_{\Omega} \left( x - \mu \right)^2 f\left( x \right) \odif{x} & \text{for continuous variables}
        \end{cases} \\
                               & = \E{\left( X^2 \right)} - \E{\left( X \right)}^2
    \end{align*}
\end{definition}
\begin{definition}[Standard deviation]
    The standard deviation is defined as
    \begin{equation*}
        \sigma = \sqrt{\Var{\left( X \right)}}
    \end{equation*}
\end{definition}
\subsubsection{Transformations}
For a simple linear function of a random variable
\begin{align*}
    \E{\left( aX \pm b \right)}   & = a\E{\left( X \right)} \pm b \\
    \Var{\left( aX \pm b \right)} & = a^2\Var{\left( X \right)}
\end{align*}
\section{Special Discrete Distributions}
\subsection{Discrete Uniform Distribution}
A discrete uniform distribution describes the probability distribution of a single trial
in a set of equally likely elements.

A discrete random variable \(X\) with a discrete uniform distribution is denoted
\begin{equation*}
    X \sim \operatorname{Uniform}{\left( a,\: b \right)}
\end{equation*}
with the PMF\@:
\begin{equation*}
    \Pr{\left( X = x \right)} = \frac{1}{b - a + 1}
\end{equation*}
for outcomes \(x \in \left\{ a,\: a + 1,\: \dots,\: b - 1,\: b \right\}\).
We can also summarise the following:
\begin{align*}
    \E{\left( X \right)}   & = \frac{a + b}{2}                           \\
    \Var{\left( X \right)} & = \frac{\left( b - a + 1 \right)^2 - 1}{12}
\end{align*}
\subsection{Bernoulli Distribution}
A Bernoulli (or binary) distribution describes the probability distribution of a Boolean-valued
outcome, i.e., success (1) or failure (0).

A discrete random variable \(X\) with a Bernoulli distribution is denoted
\begin{equation*}
    X \sim \operatorname{Bernoulli}{\left( p \right)}
\end{equation*}
with the PMF\@:
\begin{equation*}
    \Pr{\left( X = x \right)} = p^x \left( 1 - p \right)^{1 - x}
\end{equation*}
for a probability \(p \in \interval{0}{1}\) and outcomes \(x \in \left\{ 0,\: 1 \right\}\).
We can also summarise the following:
\begin{align*}
    \E{\left( X \right)}   & = p                      \\
    \Var{\left( X \right)} & = p \left( 1 - p \right)
\end{align*}
where \(\left( 1 - p \right)\) is sometimes denoted as \(q\).
\subsection{Binomial Distribution}
A binomial distribution describes the probability distribution of the number of successes
for \(n\) independent trials with the same probability of success \(p\).

A discrete random variable \(X\) with a binomial distribution is denoted
\begin{equation*}
    X \sim \operatorname{B}{\left( n,\: p \right)}
\end{equation*}
with the PMF\@:
\begin{equation*}
    \Pr{\left( X = x \right)} = \dbinom{n}{x} p^x \left( 1 - p \right)^{n - x}
\end{equation*}
for number of successes \(x \in \left\{ 0,\: 1,\: \dots,\: n \right\}\).

Here each individual trial is a Bernoulli trial, so that \(X\) can be written as the sum of
\(n\) \textit{independent and identically distributed} (iid) Bernoulli random variables, \(Y_1,\: Y_2,\: \dots,\: Y_n\).
\begin{align*}
    X & = Y_1 + Y_2 + \cdots + Y_n, & Y_i & \overset{\mathrm{iid}}{\sim} \operatorname{Bernoulli}{\left( p \right)} : \forall i \in \left\{ 1,\: 2,\: \dots,\: n \right\}.
\end{align*}
We can then summarise the following:
\begin{align*}
    \E{\left( X \right)}   & = np                      \\
    \Var{\left( X \right)} & = np \left( 1 - p \right)
\end{align*}
where \(\left( 1 - p \right)\) is sometimes denoted as \(q\).
\begin{proof}
    Given \(n\) trials, the probability of \(x\) successes will be \(p^x\).
    Similarly the probability of \(n - x\) failures will be \(\left( 1 - p \right)^{n - x}\).

    We then consider the number of ways to choose \(x\) successes out of \(n\) trials, i.e., \(\dbinom{n}{x}\).

    The intersection of these three events gives the PMF for a binomial distribution.
\end{proof}
\subsection{Geometric Distribution}
A geometric distribution describes the probability distribution of the number of trials up to and including
the first success, where each trial is independent and has the same probability of success \(p\).

A discrete random variable \(N\) with a geometric distribution is denoted
\begin{equation*}
    N \sim \operatorname{Geom}{\left( p \right)}
\end{equation*}
with the PMF\@:
\begin{equation*}
    \Pr{\left( N = n \right)} = \left( 1 - p \right)^{n - 1} p
\end{equation*}
for number of trials \(n \geq 1\).

We can also summarise the following:
\begin{align*}
    \E{\left( N \right)}   & = \frac{1}{p}       \\
    \Var{\left( N \right)} & = \frac{1 - p}{p^2}
\end{align*}
\subsubsection{Alternate Geometric Definition}
We can alternatively consider the number of failures until a success, \(Y\):
\begin{equation*}
    Y = N - 1
\end{equation*}
The PMF for \(Y\) is:
\begin{equation*}
    \Pr{\left( Y = y \right)} = \left( 1 - p \right)^y p
\end{equation*}
for number of failures \(y \geq 0\). This gives the following summary statistics using
transformation rules:
\begin{align*}
    \E{\left( Y \right)}   & = \frac{1 - p}{p}   \\
    \Var{\left( Y \right)} & = \frac{1 - p}{p^2}
\end{align*}
\subsection{Negative Binomial Distribution}
A negative binomial distribution describes the probability distribution of the number of trials until \(k \geq 1\)
successes, where each trial is independent and has the same probability of success \(p\).

A discrete random variable \(N\) with a negative binomial distribution is denoted
\begin{equation*}
    N \sim \operatorname{NB}{\left( k,\: p \right)}
\end{equation*}
with the PMF\@:
\begin{align*}
    \Pr{\left( N = n \right)} & = \dbinom{n - 1}{k - 1} \left( 1 - p \right)^{n - k} p^k
\end{align*}
for number of trials \(n \geq k\).
Here each individual trial is a Geometric trial, so that \(N\) can be written as the sum of
\(k\) \textit{independent and identically distributed} (iid) Geometric random variables, \(Y_1,\: Y_2,\: \dots,\: Y_k\).
\begin{align*}
    N & = Y_1 + Y_2 + \cdots + Y_k, & Y_i & \overset{\mathrm{iid}}{\sim} \operatorname{Geom}{\left( p \right)} : \forall i \in \left\{ 1,\: 2,\: \dots,\: k \right\}.
\end{align*}
We can then summarise the following:
\begin{align*}
    \E{\left( N \right)}   & = \frac{k}{p}                        \\
    \Var{\left( N \right)} & = \frac{k \left( 1 - p \right)}{p^2}
\end{align*}
\begin{proof}
    Given \(n\) trials, the probability of \(k\) successes will be \(p^k\).
    Similarly the probability of \(n - k\) failures will be \(\left( 1 - p \right)^{n - k}\).

    We then consider the number of ways to arrange \(k - 1\) successes for \(n - 1\) trials,
    because the last trial must be a success, i.e., \(\dbinom{n - 1}{k - 1}\).

    The intersection of these three events gives the PMF for a negative binomial distribution.
\end{proof}
\subsubsection{Alternate Negative Binomial Definition}
We can alternatively consider the number of failures until \(k\) successes, \(Y\):
\begin{equation*}
    Y = N - k
\end{equation*}
The PMF for \(Y\) is:
\begin{equation*}
    \Pr{\left( Y = y \right)} = \dbinom{y + k - 1}{k - 1} \left( 1 - p \right)^y p^k
\end{equation*}
for number of failures \(y \geq 0\). This gives the following summary statistics using
transformation rules:
\begin{align*}
    \E{\left( Y \right)}   & = \frac{k \left( 1 - p \right)}{p}   \\
    \Var{\left( Y \right)} & = \frac{k \left( 1 - p \right)}{p^2}
\end{align*}
\subsection{Poisson Distribution}
A Poisson distribution describes the probability distribution of the number of events \(N\) which occur over a fixed interval of time \(\lambda\).

A discrete random variable \(N\) with a Poisson distribution is denoted
\begin{equation*}
    N \sim \operatorname{Pois}{\left( \lambda \right)}
\end{equation*}
with the PMF\@:
\begin{equation*}
    \Pr{\left( N = n \right)} = \frac{\lambda^n e^{-\lambda}}{n!}
\end{equation*}
for number of events \(n \geq 0\).
We can also summarise the following:
\begin{align*}
    \E{\left( N \right)}   & = \lambda \\
    \Var{\left( N \right)} & = \lambda
\end{align*}
\end{document}
